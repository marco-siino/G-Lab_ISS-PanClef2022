{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/G-Lab_ISS-PanClef2022/blob/main/G-Lab_ISS2022_DataEnhancement%2BCNN_ModelNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBqUcj4cx2G"
      },
      "source": [
        "## Importing modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQSunQ-ucjLX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from google.colab import files\n",
        "from io import open\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QHd_fxmHCfa"
      },
      "source": [
        "## Importing DS and extract in current working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocYMUXaY8r0_",
        "outputId": "fec0d0bf-6bd5-4a34-dafc-c317218159c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/marco-siino/iss/raw/main/pan22-author-profiling-training-2022-03-29-augmented.zip\n",
            "9961472/9960878 [==============================] - 0s 0us/step\n",
            "9969664/9960878 [==============================] - 0s 0us/step\n",
            "Downloading data from https://github.com/marco-siino/iss/raw/main/pan22-author-profiling-test-2022-04-22-without_truth-augmented.zip\n",
            "4161536/4154540 [==============================] - 0s 0us/step\n",
            "4169728/4154540 [==============================] - 0s 0us/step\n",
            "./pan22-author-profiling-training-2022-03-29-augmented.zip\n",
            "./pan22-author-profiling-training-2022-03-29-augmented\n",
            ".config\n",
            "pan22-author-profiling-test-2022-04-22-without_truth-augmented\n",
            "pan22-author-profiling-test-2022-04-22-without_truth-augmented.zip\n",
            "pan22-author-profiling-training-2022-03-29-augmented\n",
            "pan22-author-profiling-training-2022-03-29-augmented.zip\n",
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "# Url obtained starting from this: https://drive.google.com/file/d/19ZcqEv88euKB71HfAWjTGN3uCKp2qsfP/ and forcing export=download.\n",
        "train_set_url = \"https://github.com/marco-siino/iss/raw/main/pan22-author-profiling-training-2022-03-29-augmented.zip\"\n",
        "test_set_url=\"https://github.com/marco-siino/iss/raw/main/pan22-author-profiling-test-2022-04-22-without_truth-augmented.zip\"\n",
        "\n",
        "train_set_path = tf.keras.utils.get_file(\"pan22-author-profiling-training-2022-03-29-augmented.zip\", train_set_url,\n",
        "                                    extract=True, archive_format='zip',cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "test_set_path = tf.keras.utils.get_file(\"pan22-author-profiling-test-2022-04-22-without_truth-augmented.zip\", test_set_url,\n",
        "                                  extract=True, archive_format='zip',cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "train_set_dir = os.path.join(os.path.dirname(train_set_path), 'pan22-author-profiling-training-2022-03-29-augmented')\n",
        "test_set_dir = os.path.join(os.path.dirname(test_set_path), 'pan22-author-profiling-test-2022-04-22-without_truth-augmented')\n",
        "\n",
        "print(train_set_path)\n",
        "print(train_set_dir)\n",
        "\n",
        "!ls -A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di7jOZjALo4X"
      },
      "source": [
        "## Build folders hierarchy to use Keras folders preprocessing function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ucATWhfGSGf",
        "outputId": "dd5225a3-0253-4267-857a-4ba1d04c4928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".config\n",
            "pan22-author-profiling-test-2022-04-22-without_truth-augmented\n",
            "pan22-author-profiling-test-2022-04-22-without_truth-augmented.zip\n",
            "pan22-author-profiling-training-2022-03-29-augmented\n",
            "pan22-author-profiling-training-2022-03-29-augmented.zip\n",
            "sample_data\n",
            "train_dir\n"
          ]
        }
      ],
      "source": [
        "### Training Folders. ###\n",
        "# First level directory.\n",
        "if not os.path.exists('train_dir'):\n",
        "    os.makedirs('train_dir')\n",
        "\n",
        "# Class labels directory.\n",
        "if not os.path.exists('train_dir/0'):\n",
        "    os.makedirs('train_dir/0')\n",
        "if not os.path.exists('train_dir/1'):\n",
        "    os.makedirs('train_dir/1')\n",
        "\n",
        "!ls -A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNib56hF_8an"
      },
      "source": [
        "## Set language and directory paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq0EgZuf_5tv"
      },
      "outputs": [],
      "source": [
        "# Set train_dir and test_dir paths.\n",
        "#truth_file_test_dir=test_set_dir\n",
        "\n",
        "truth_file_train_dir=train_set_dir+'/'\n",
        "truth_file_training_path = truth_file_train_dir+'en.txt'\n",
        "\n",
        "#truth_file_test_path_en = truth_file_test_dir+'/'+language+'.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VQKsc4XOpD8"
      },
      "source": [
        "## Read truth.txt to organize training dataset folders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kxcJ92-Nkto"
      },
      "outputs": [],
      "source": [
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_training_path, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "    # Change Classes NI->0 and I->1.\n",
        "    if (label=='I'):\n",
        "      label=1\n",
        "    else:\n",
        "      label=0\n",
        "   \n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_train_dir+'en/'+fNameXml):\n",
        "      os.rename(truth_file_train_dir+'en/'+fNameXml, './train_dir/'+str(label)+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRMgBDgsCua6"
      },
      "source": [
        "## Building the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEu8rdcQCuHy",
        "outputId": "c983efe7-7f93-486b-9a18-8e3a34a64e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 420 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size=1  \n",
        "\n",
        "full_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train_dir', \n",
        "    batch_size=batch_size, \n",
        "    shuffle=False,\n",
        "    seed=1\n",
        "    )\n",
        "\n",
        "full_train_ds=full_train_ds.shuffle(420,seed=1, reshuffle_each_iteration=False)\n",
        "full_train_ds_size=len(full_train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQxej8gL9GLQ"
      },
      "source": [
        "## First model's layer: Text Vectorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWh2tZYGYXs4"
      },
      "outputs": [],
      "source": [
        "# Function to generate a text_vectorization_layer.\n",
        "def gen_text_vectorization_layer(train_set):\n",
        "    # Set a very large sequence length to find the longest sample.\n",
        "    sequence_length = 30000\n",
        "    vectorize_layer_tmp = TextVectorization(\n",
        "        standardize=None,\n",
        "        output_mode='int',\n",
        "        output_sequence_length=sequence_length)\n",
        "\n",
        "    train_text = train_set.map(lambda x, y: x)\n",
        "    vectorize_layer_tmp.adapt(train_text)\n",
        "    #vectorize_layer.get_vocabulary()\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "    model.add(vectorize_layer_tmp)\n",
        "\n",
        "    longest_sample_length=1\n",
        "    max_nr_dictionary_entry = 1\n",
        "\n",
        "    for element in train_set:\n",
        "      authorDocument=element[0]\n",
        "      label=element[1]\n",
        "      \n",
        "      #print(\"Sample considered is: \", authorDocument[0].numpy())\n",
        "      #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "      #print(\"And has label: \", label[0].numpy())\n",
        "\n",
        "      # Count the number of zeros from the last non-zero token to the end of the sample. \n",
        "      # Shortest tokenized sample has less zeros than others.\n",
        "      out=model(authorDocument)\n",
        "      token_nr_index=sequence_length-1\n",
        "      current_sample_zeros_counter=0\n",
        "      while out.numpy()[0][token_nr_index]==0:\n",
        "        token_nr_index-=1\n",
        "        current_sample_zeros_counter+=1\n",
        "\n",
        "      shortest_padding_length=sequence_length-longest_sample_length\n",
        "      if current_sample_zeros_counter<shortest_padding_length:\n",
        "        longest_sample_length=sequence_length-current_sample_zeros_counter\n",
        "\n",
        "      # Get greater token value (to set dictionary size) for current sample.\n",
        "      if np.amax(out.numpy()[0])>max_nr_dictionary_entry:\n",
        "        max_nr_dictionary_entry=np.amax(out.numpy()[0])\n",
        "\n",
        "    # Be sure to include each ngram.\n",
        "    max_nr_dictionary_entry+=1\n",
        "\n",
        "    print(\"Length of the longest sample in train set is:\",longest_sample_length)\n",
        "    print(\"Dictionary size is:\",max_nr_dictionary_entry)\n",
        "\n",
        "    sequence_length = longest_sample_length\n",
        "\n",
        "    vectorize_layer = TextVectorization(\n",
        "        standardize=None,\n",
        "        max_tokens=max_nr_dictionary_entry,\n",
        "        output_mode='int',\n",
        "        output_sequence_length=sequence_length)\n",
        "    train_text = train_set.map(lambda x, y: x)\n",
        "    vectorize_layer.adapt(train_text)\n",
        "    \n",
        "    return vectorize_layer,max_nr_dictionary_entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJc08u4D4V11"
      },
      "source": [
        "## Split the full training set to do a 5-cross fold validation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11Qjgk6a4X2g"
      },
      "outputs": [],
      "source": [
        "# 5 Cross fold generation example. \n",
        "\n",
        "# 1° Fold -> 80% - 20%V\n",
        "# 2° Fold -> 60% - 20%V - 20%\n",
        "# 3° Fold -> 40% - 20%V - 40%\n",
        "# 4° Fold -> 20% - 20%V - 60%\n",
        "# 5° Fold -> 20%V - 80%\n",
        "\n",
        "train=[]\n",
        "val=[]\n",
        "test = []\n",
        "\n",
        "# train_ds = train+val. DS used for model development.\n",
        "full_train_ds_size = len(full_train_ds)\n",
        "train_ds = full_train_ds.take(380)\n",
        "# Final set to be used at the end of development phase. A little less than 10% of the original ds (420 samples).\n",
        "test_set = full_train_ds.skip(380)\n",
        "\"\"\"\n",
        "        380           40        TOT = 420\n",
        "|_________________| |_____|\n",
        "      train_ds      test_set\n",
        "\"\"\"\n",
        "\n",
        "# Percentage start and end of validation subset within train_ds. (at each fold: train->304 samples, val->76 samples)\n",
        "val_percentage_start=80\n",
        "val_percentage_end=100\n",
        "val_percentage_size=20\n",
        "fold_nr=5\n",
        "\n",
        "for i in range(0,fold_nr):\n",
        "  train.append(train_ds.take(int(len(train_ds)*val_percentage_start/100)))\n",
        "  train[i] = train[i].concatenate(train_ds.skip(int(len(train_ds)*val_percentage_end/100)))\n",
        "\n",
        "  val.append(train_ds.skip(int(len(train_ds)*val_percentage_start/100)))\n",
        "  val[i] = val[i].take(int(len(train_ds)*val_percentage_size/100))\n",
        "\n",
        "  val_percentage_start-=val_percentage_size\n",
        "  val_percentage_end-=val_percentage_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgvX5p5QB24X"
      },
      "source": [
        "## CNN model definition and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcbadi5dBF8J",
        "outputId": "30b31f67-a091-4920-b7e3-351898e0d875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Fold nr.:  0\n",
            "Length of the longest sample in train set is: 20382\n",
            "Dictionary size is: 165944\n",
            "Epoch 1/5\n",
            "304/304 [==============================] - 17s 21ms/step - loss: 0.6794 - binary_accuracy: 0.5625 - val_loss: 0.5255 - val_binary_accuracy: 0.7763\n",
            "Epoch 2/5\n",
            "304/304 [==============================] - 6s 21ms/step - loss: 0.3280 - binary_accuracy: 0.8750 - val_loss: 0.3926 - val_binary_accuracy: 0.8026\n",
            "Epoch 3/5\n",
            "304/304 [==============================] - 7s 21ms/step - loss: 0.0748 - binary_accuracy: 0.9671 - val_loss: 0.2648 - val_binary_accuracy: 0.8947\n",
            "Epoch 4/5\n",
            "304/304 [==============================] - 7s 21ms/step - loss: 0.0178 - binary_accuracy: 0.9967 - val_loss: 0.3334 - val_binary_accuracy: 0.9079\n",
            "Epoch 5/5\n",
            "304/304 [==============================] - 6s 21ms/step - loss: 0.0026 - binary_accuracy: 1.0000 - val_loss: 0.3322 - val_binary_accuracy: 0.8947\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2981 - binary_accuracy: 0.9000\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.3322 - binary_accuracy: 0.8947\n",
            "AVG accuracy on Val and Test set is: 0.8973684012889862\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.2981 - binary_accuracy: 0.9000\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.3322 - binary_accuracy: 0.8947\n",
            "\n",
            "\n",
            "Fold nr.:  1\n",
            "Length of the longest sample in train set is: 20382\n",
            "Dictionary size is: 165152\n",
            "Epoch 1/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.6673 - binary_accuracy: 0.5559 - val_loss: 0.5328 - val_binary_accuracy: 0.7632\n",
            "Epoch 2/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.3361 - binary_accuracy: 0.8684 - val_loss: 0.2343 - val_binary_accuracy: 0.9079\n",
            "Epoch 3/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.0755 - binary_accuracy: 0.9704 - val_loss: 0.2821 - val_binary_accuracy: 0.9342\n",
            "Epoch 4/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.0174 - binary_accuracy: 0.9934 - val_loss: 0.3015 - val_binary_accuracy: 0.8553\n",
            "Epoch 5/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.0022 - binary_accuracy: 1.0000 - val_loss: 0.2710 - val_binary_accuracy: 0.8684\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.2995 - binary_accuracy: 0.8750\n",
            "76/76 [==============================] - 1s 6ms/step - loss: 0.2710 - binary_accuracy: 0.8684\n",
            "AVG accuracy on Val and Test set is: 0.8717105388641357\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.2995 - binary_accuracy: 0.8750\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.2710 - binary_accuracy: 0.8684\n",
            "\n",
            "\n",
            "Fold nr.:  2\n",
            "Length of the longest sample in train set is: 20153\n",
            "Dictionary size is: 165393\n",
            "Epoch 1/5\n",
            "304/304 [==============================] - 8s 23ms/step - loss: 0.6694 - binary_accuracy: 0.5559 - val_loss: 0.5363 - val_binary_accuracy: 0.7895\n",
            "Epoch 2/5\n",
            "304/304 [==============================] - 7s 23ms/step - loss: 0.3128 - binary_accuracy: 0.8750 - val_loss: 0.2813 - val_binary_accuracy: 0.9211\n",
            "Epoch 3/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.0667 - binary_accuracy: 0.9671 - val_loss: 0.3623 - val_binary_accuracy: 0.8947\n",
            "Epoch 4/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.0166 - binary_accuracy: 0.9934 - val_loss: 0.3547 - val_binary_accuracy: 0.9079\n",
            "Epoch 5/5\n",
            "304/304 [==============================] - 7s 23ms/step - loss: 0.0019 - binary_accuracy: 1.0000 - val_loss: 0.3024 - val_binary_accuracy: 0.9211\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.3091 - binary_accuracy: 0.9250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.3024 - binary_accuracy: 0.9211\n",
            "AVG accuracy on Val and Test set is: 0.9230263233184814\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.3091 - binary_accuracy: 0.9250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.3024 - binary_accuracy: 0.9211\n",
            "\n",
            "\n",
            "Fold nr.:  3\n",
            "Length of the longest sample in train set is: 20382\n",
            "Dictionary size is: 165420\n",
            "Epoch 1/5\n",
            "304/304 [==============================] - 8s 23ms/step - loss: 0.6564 - binary_accuracy: 0.5757 - val_loss: 0.5502 - val_binary_accuracy: 0.7105\n",
            "Epoch 2/5\n",
            "304/304 [==============================] - 7s 23ms/step - loss: 0.2999 - binary_accuracy: 0.8914 - val_loss: 0.3439 - val_binary_accuracy: 0.8158\n",
            "Epoch 3/5\n",
            "304/304 [==============================] - 7s 23ms/step - loss: 0.0764 - binary_accuracy: 0.9704 - val_loss: 0.4722 - val_binary_accuracy: 0.8421\n",
            "Epoch 4/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.0184 - binary_accuracy: 0.9967 - val_loss: 0.4055 - val_binary_accuracy: 0.8553\n",
            "Epoch 5/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.0031 - binary_accuracy: 1.0000 - val_loss: 0.4102 - val_binary_accuracy: 0.8684\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.2371 - binary_accuracy: 0.9250\n",
            "76/76 [==============================] - 1s 6ms/step - loss: 0.4102 - binary_accuracy: 0.8684\n",
            "AVG accuracy on Val and Test set is: 0.8967105448246002\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.2371 - binary_accuracy: 0.9250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.4102 - binary_accuracy: 0.8684\n",
            "\n",
            "\n",
            "Fold nr.:  4\n",
            "Length of the longest sample in train set is: 20382\n",
            "Dictionary size is: 167808\n",
            "Epoch 1/5\n",
            "304/304 [==============================] - 8s 23ms/step - loss: 0.6598 - binary_accuracy: 0.6053 - val_loss: 0.5760 - val_binary_accuracy: 0.7368\n",
            "Epoch 2/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.3423 - binary_accuracy: 0.8618 - val_loss: 0.2302 - val_binary_accuracy: 0.9079\n",
            "Epoch 3/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.0723 - binary_accuracy: 0.9770 - val_loss: 0.2265 - val_binary_accuracy: 0.9079\n",
            "Epoch 4/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.0121 - binary_accuracy: 0.9967 - val_loss: 0.2217 - val_binary_accuracy: 0.9079\n",
            "Epoch 5/5\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 0.0024 - binary_accuracy: 1.0000 - val_loss: 0.2129 - val_binary_accuracy: 0.9079\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.1939 - binary_accuracy: 0.9250\n",
            "76/76 [==============================] - 1s 6ms/step - loss: 0.2129 - binary_accuracy: 0.9079\n",
            "AVG accuracy on Val and Test set is: 0.9164473712444305\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.1939 - binary_accuracy: 0.9250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.2129 - binary_accuracy: 0.9079\n",
            "\n",
            "\n",
            "******************************************************\n",
            "\n",
            "\n",
            "Over five folds AVG accuracy is: 0.9010526359081268\n"
          ]
        }
      ],
      "source": [
        "# Word embedding dimensions.\n",
        "embedding_dim = 100\n",
        "cnn = []\n",
        "max_features = []\n",
        "\n",
        "cnn_preds_results =[]\n",
        "\n",
        "for current_fold in range(0,fold_nr):\n",
        "  print(\"\\n\\nFold nr.: \", current_fold)\n",
        "\n",
        "  vectorize_layer, max_features_tmp = gen_text_vectorization_layer(train[current_fold])\n",
        "  max_features.append(max_features_tmp)\n",
        "\n",
        "  cnn.append(tf.keras.Sequential([\n",
        "                              tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "                              vectorize_layer,\n",
        "                              layers.Embedding(max_features[current_fold] + 1, embedding_dim),                     \n",
        "                              layers.Dropout(0.4),\n",
        "\n",
        "                              layers.Conv1D(64,16),\n",
        "                              layers.MaxPooling1D(),\n",
        "                              layers.Dropout(0.5),                 \n",
        "                              \n",
        "                              layers.GlobalAveragePooling1D(),\n",
        "                              layers.Dense(1)\n",
        "    ]))\n",
        "\n",
        "  opt = tf.keras.optimizers.Adam()\n",
        "  cnn[current_fold].compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer=opt, metrics=tf.metrics.BinaryAccuracy(threshold=0.0)) \n",
        "\n",
        "  epochs = 5\n",
        "  history = cnn[current_fold].fit(\n",
        "      train[current_fold],\n",
        "      validation_data=val[current_fold],\n",
        "      epochs=epochs,\n",
        "      shuffle=False,\n",
        "      # Comment the following line to do not save and download the model.\n",
        "      #callbacks=[callbacks]\n",
        "      )\n",
        "  \n",
        "  \n",
        "  print(\"AVG accuracy on Val and Test set is:\", str((cnn[current_fold].evaluate(test_set)[1]+cnn[current_fold].evaluate(val[current_fold])[1])/2))\n",
        "  cnn_preds_results.append((cnn[current_fold].evaluate(test_set)[1]+cnn[current_fold].evaluate(val[current_fold])[1])/2)\n",
        "  #model_en.summary()\n",
        "print(\"\\n\\n******************************************************\")\n",
        "print(\"\\n\\nOver five folds AVG accuracy is:\", str(sum(cnn_preds_results)/fold_nr))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Generate predictions and write into separate XML files. (1 file -> 1 author prediction)\n",
        "\n",
        "# msiino_iss2022task_predictions/en/aId.xml\n",
        "if not os.path.exists('glab_iss2022task_predictions'):\n",
        "    os.makedirs('glab_iss2022task_predictions')\n",
        "if not os.path.exists('glab_hss2021task_predictions/en/'):\n",
        "    os.makedirs('glab_hss2021task_predictions/en/')\n",
        "\n",
        "languages=['en']\n",
        "prediction_counter=0\n",
        "for language in languages: \n",
        "  for filename in os.listdir('pan22-author-profiling-test-2022-04-22-without_truth-augmented/en/'):\n",
        "    if filename!=\".txt\":\n",
        "      prediction_counter+=1\n",
        "      print(\"\\nFile nr.: \", prediction_counter)\n",
        "      \n",
        "      x = filename.split(\".\")\n",
        "      \n",
        "      author_id = x[0]\n",
        "      print(\"Filename:\", filename)\n",
        "\n",
        "      if not os.path.exists('tmp_test_author_dir/'+author_id):\n",
        "        os.makedirs('tmp_test_author_dir/'+author_id)\n",
        "      \n",
        "      if os.path.exists('pan22-author-profiling-test-2022-04-22-without_truth-augmented/en/'+filename):\n",
        "        shutil.copyfile('pan22-author-profiling-test-2022-04-22-without_truth-augmented/en/'+filename, 'tmp_test_author_dir/'+author_id+'/'+author_id+'.txt' )\n",
        "\n",
        "      test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "        'tmp_test_author_dir', \n",
        "        batch_size=1,\n",
        "        shuffle=False\n",
        "        )\n",
        "      \n",
        "      for current_sample in train[1]:\n",
        "        print(current_sample[0])\n",
        "        print(cnn[1].predict(current_sample[0])[0][0])\n",
        "        if cnn[1].predict(current_sample[0])[0][0]>0.0:\n",
        "          prediction=\"I\"\n",
        "        else:\n",
        "          prediction=\"NI\"\n",
        "        #prediction = model.predict_classes(current_sample)[0][0]\n",
        "      print(\"Author id:\",author_id)\n",
        "      print(\"Language:\", language)\n",
        "      print(\"Class predicted:\", prediction)\n",
        "      print(\"Model output: \", cnn[1].predict(current_sample[0]))\n",
        "      xml_content= \"<author id=\\\"\" + author_id + \"\\\" lang=\\\"\" + language + \"\\\" type=\\\"\" + str(prediction) + \"\\\" />\"\n",
        "\n",
        "      f = open(\"glab_iss2022task_predictions/\"+author_id+\".xml\", \"a\")\n",
        "      f.write(xml_content)\n",
        "      f.close()\n",
        "\n",
        "      shutil.rmtree('tmp_test_author_dir/'+author_id)\n",
        "\n",
        "## Zip and Download the predictions (remember to set Callback to use this!).\n",
        "\n",
        "!zip -r glab_iss2022task_predictions.zip glab_iss2022task_predictions\n",
        "# If automatic download doesn't start, open the directory browser on the left menu and download the zip file manually.\n",
        "files.download(\"glab_iss2022task_predictions.zip\")"
      ],
      "metadata": {
        "id": "E_heobaE6Cy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6IBqUcj4cx2G",
        "0QHd_fxmHCfa",
        "Di7jOZjALo4X",
        "tNib56hF_8an",
        "7VQKsc4XOpD8",
        "MRMgBDgsCua6",
        "BQxej8gL9GLQ",
        "TJc08u4D4V11"
      ],
      "name": "G-Lab_ISS2022_DataEnhancement+CNN_ModelNB",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}